%        File: test.tex
%     Created: Thu Apr 21 03:00 PM 2011 C
% Last Change: Thu Apr 21 03:00 PM 2011 C
%

\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}

\section{Method}
The core idea is to find a set of axis aligned hyperplanes that maximizes the area under the resulting ROC curve. Since the hyperplanes generates a set of $k$-dimensional cells which makes it possible to treat each sample as a binary vector as in the discrete case. We have the following input
\begin{itemize}
  \item Values $x_{i,l} \in \mathbb{R}$ (e.g. an individual SNP and risk factors) where $i \in [m]$ and $l \in [k]$.
  \item Classification $p_i \in {0,1}$ where $i \in [m]$.
  \item Let $I_1 = \{ i \in [m] | p_i = 1 \}$ and $I_0 = \{ i \in [m] | p_i = 0 \}$.
\end{itemize}
\subsection{Area under the ROC curve}
The Area Under the Curve (AUC) for the ROC curve is the probability that a positive sample is predicted as a positive sample. Formally we state it as follows
\begin{equation}\label{eq:auroc}
AUC = Pr[h(X^+) > h(X^-)]
\end{equation}
where $X^+$ and $X^-$ is random variables from the positive and negative distribution respectively, and $h$ is a hypothesis function that ranks the samples. In the discrete case a sample was ranked according to the fraction of positive to negative samples for a given assignment of risk factors. The continuous case is handled analogously, they are ranked according to the fraction of positive to negative samples in a given cell generated by the hyperplanes. For a random variable we define the hypothesis function for the continuous case as follows
$$h(X) = \frac{E[I_{ \{c(X^+) = c(X)\} }] }{ E[I_{ \{c(X^-) = c(X)\} }] } = \frac{Pr[c(X^+) = c(X)]}{Pr[c(X^-)=c(X)]} $$
where $c(X)$ is a function that determines which cell $X$ belongs to, of course when $X$ is random so is $c(X)$.

We note that equation (\ref{eq:auroc}) can be approximated by the law of large numbers
\begin{equation}\label{eq:approx}
Pr[h(X^+) > h(X^-)] = E[I_{\{h(X^+)\ >\ h(X^-)\}}] \approx \frac{ \sum_{i \in I_1}\sum_{j \in I_0} I_{\{h(x_i^+) > h(x_j^-)\} }}{|I_1| \cdot |I_0|}
\end{equation}
where $x_i^+$ and $x_j^-$ is the $i$:th positive and $j$:th negative sample. Will we now use linear programming to position the hyperplanes such that (\ref{eq:approx}) is maximized.
\subsection{The linear programming problem}
A linear programming problem has the following form:
\begin{align*}
&\text{Maximize} & f(\mathbf{x}) = \mathbf{c}^T\mathbf{x}\\
&\text{subject to} & A\mathbf{x} \leq b\\
&\text{and} & \mathbf{x} \geq 0
\end{align*}
\subsection{Linear programming formulation}
In our case we will have (\ref{eq:approx}) as our $f$ and $\mathbf{x}$. The basis of our formulation is a set of variables $n_{i,l}$ that specifies which side of the hyperplane individual $i$ is in dimension $l$, $0$ means left and $1$ means right. We assume here that in an optimal solution each $n_{i,l}$ will be either $0$ or $1$. The constraint for the variables are
\begin{itemize}
  \item $0 \leq n_{i_1,l} \leq \dots \leq n_{i_m,l} \leq 1$ where $i_1, \dots, i_m$ is a permutation such that $x_{i_1,l} \leq \dots \leq x_{i_m,l}$. This conditions says that the values projected down to dimension $l$ will be split into two intervals, and also that each $n_{i,l}$ must be between $0$ and $1$ (hopefully either $0$ or $1$).
\end{itemize}
Since we are restricted to a linear function we cannot optimize (\ref{eq:approx}) directly. It can be shown that it is equivalent to maximize the probability that positive and negative samples are in different cells
$$ Pr[c(X^+) \neq c(X^-)] $$
see the proof in section 3.
Now given an assignment of the $n_{i,l}$ variables we need to define an indicator function that determines if two variables are in the same cell. Since we only are allowed to use linear functions this is a bit tricky, and involves introducing a set of slack variables, a common technique in linear programming. To simplify notation let
$$s_i = \sum_{l = 1}^{k} 2^{l-1} n_{i,l}$$
i.e. a projection from a binary vector determining the cell to the corresponding real number. Now we introduce two slack variables for determining which of $s_i$ and $s_j$ is largest by adding two terms to the function $f$ and a set of constraints.
\begin{itemize}
  \item Let $g_{i,j} \geq 0$  and $l_{i,j} \geq 0$ represent $g_{i,j} = max(s_i, s_j)$ and $l_{i,j} = min(s_i, s_j)$, by the conditions $l_{i,j} \geq s_i$, $l_{i,j} \geq s_j$ and $g_{i,j} \leq s_i$, $g_{i,j} \leq s_j$.
\end{itemize}
Using these we can define $e_{i,j}$ to approximate the indicator function.
\begin{itemize}  
  \item Let $e_{i,j}$ have the following conditions $0 \leq e_{i,j} \leq 1$ and $e_{i,j} \leq g_{i,j} - l_{i,j}$. This means that if $s_i$ and $s_j$ is the same $e_{i,j}$ will become zero, however if they are different $g_{i,j} - l_{i,j} \geq 1$ and it will become $1$. 
\end{itemize}
Finally $f$ is formulated as
$$ f(\mathbf{e},\mathbf{l},\mathbf{g}) = \sum_{i \in I_1}\sum_{j \in I_0} e_{i,j} + K\left(\sum_{i \in I_1}\sum_{j \in I_0} g_{i,j} - \sum_{i \in I_1}\sum_{j \in I_0}l_{i,j}\right)$$
where the last two sums are there so that the slack variables $g$ and $l$ represent the $min$ and $max$ functions. It is important to choose $K$ such that the sum is dominated by the first term.
\section{Proof of equivalent maximization}
\subsection{Theorem}
Let $C$ represent a set of cells in $\mathbb{R}^k$ such that $\cup_{c \in C} c = \mathbb{R}^k$. Then for the random variables $X^+$ and $X^-$ the following is true
$$ \operatorname*{arg\,max}_{C} Pr[c(X^+) \neq c(X^-)] = \operatorname*{arg\,max}_{C} Pr[h(X^+) > h(X^-)]$$
where
$$ h(X) =  \frac{Pr[c(X^+) = c(X)]}{Pr[c(X^-)=c(X)]}$$
\subsection{Proof}
The left side can be rewritten as follows
$$Pr[c(X^+) \neq c(X^-)] = \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[c(X^+) = c_1, c(X^-) = c_2] $$
since the left side is just a union of disjoint events.
The right side can similarly be rewritten
$$ Pr[h(X^+) > h(X^-)] = $$
$$ = \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[h(X^+) > h(X^-) | c(X^+) = c_1, c(X^-) = c_2]Pr[c(X^+) = c_1, c(X^-) = c_2]$$
$$ = \sum_{c_1,c_2 \in C, c_1 \neq c_2} I_{ \frac{Pr[c(X^+) = c_1]}{Pr[c(X^-) = c_1]} > \frac{Pr[c(X^+) = c_2]}{Pr[c(X^-) = c_2]} } Pr[c(X^+) = c_1, c(X^-) = c_2] =$$
$$ = \frac{1}{2} \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[c(X^+) = c_1, c(X^-) = c_2] $$
The last step is  true since we are trying all pairs of cells and in one direction the indicator function must be true. Except on a factor $\frac{1}{2}$ the functions are the same, thus the theorem holds.
\end{document}
