%        File: test.tex
%     Created: Thu Apr 21 03:00 PM 2011 C
% Last Change: Thu Apr 21 03:00 PM 2011 C
%

\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}
\section{Introduction}
We want to investigate how the phenotype of an individual depends on the interaction between a set of SNPs and environmental factors. Due to computational difficulties we will restrict ourselves to a single SNP and a subset of the environmental factors. 

\section{Notation}
For an integer $n$ let $[n]$ denote the set $\{1,...,n\}$. Let $N$ be the number of individuals. Let $P$ and $C$ be the set of indices of the individuals for the patients (cases) and controls respectively, such that $P \cup C = [N]$. We use the term "risk factors" to collectively refer to the genetic (SNPs) and environmental factors. An individual $i$ has $D$ risk factors denoted by $r_{i,d}$ where $d \in [D]$. Let $\pi^d(i)$ be a permutation such that $r_{\pi^d(1),d} \leq r_{\pi^d(2),d} \leq \dots \leq r_{\pi^d(N),d}$, i.e. it sorts the values of risk factors in dimension $d$. For a vector $v \in \mathbb{R}^n$ let $\mathbf{e}_v$ be a vector of the same length $n$ containing only ones. For a vector $v \in \mathbb{R}^n$ the transpose is denoted $v^T$.
\section{Method}
As we saw in the discrete case we merely find combinations of factors that have a high case control ratio, such that we can construct a ROC curve. We handle the continuous case similarly by finding regions, or cells, separated by hyperplanes that results in a good ROC curve. Formally, we want to find a set of hyperplanes, one orthogonal hyperplane to each dimension, that maximizes the area under the resulting ROC curve. The $D$-dimensional cells then makes it possible to encode the risk factors as binary vectors analogously to the discrete case.
\subsection{Area under the ROC curve}
To be able to formulate the problem we need some prior knowledge on the Area Under the Curve (AUC) for the ROC curve. The AUC is the probability that a positive sample is predicted as a positive sample. Formally we state it as follows
\begin{equation}\label{eq:auroc}
AUC = Pr[h(X^+) > h(X^-)]
\end{equation}
where $X^+$ and $X^-$ is random variables from the case and control distribution respectively, and $h$ is a hypothesis function that ranks the samples. In the discrete case a sample was ranked according to the fraction of cases to controls samples for a given assignment of risk factors. The continuous case is handled analogously, they are ranked according to the fraction of cases to controls in a given cell generated by the hyperplanes. For a random variable we define the hypothesis function for the continuous case as follows
$$h(X) = \frac{E[I_{ \{c(X^+) = c(X)\} }] }{ E[I_{ \{c(X^-) = c(X)\} }] } = \frac{Pr[c(X^+) = c(X)]}{Pr[c(X^-)=c(X)]} $$
where $c(X)$ is a function that determines which cell $X$ belongs to, of course when $X$ is random so is $c(X)$.

We note that equation (\ref{eq:auroc}) can be approximated by the law of large numbers
\begin{equation}\label{eq:approx}
Pr[h(X^+) > h(X^-)] = E[I_{\{h(X^+)\ >\ h(X^-)\}}] \approx \frac{ \sum_{i \in P}\sum_{j \in C} I_{\{h(x_i) > h(x_j)\} }}{|P| \cdot |C|}
\end{equation}
where $x_i$ and $x_j$ is the $i$:th positive and $j$:th negative sample. Will we now show how to use quadratic programming to position the hyperplanes such that (\ref{eq:approx}) is maximized.
\subsection{Quadratic programming}
Quadratic programming is the process of finding a vector that minimizes a convex quadratic function subject to some constraints. The standard form of a quadratic programming problem is:
\begin{align*}
&\text{Minimize} & f(\mathbf{x}) = \mathbf{x}^TQ\mathbf{x} + \mathbf{c}^T\mathbf{x}\\
&\text{subject to} & A\mathbf{x} \leq \mathbf{b}\\
&\text{and} & \mathbf{x} \geq 0
\end{align*}
where $\mathbf{x},\mathbf{c},\mathbf{b} \in \mathbb{R}^n$ and $Q,A \in \mathbb{R}^{n \times n}$. For the problem to be to be feasible $Q$ must be positive semidefinite. This just means that the function $f$ is nice in the sense that every local minimum is also a global minimum (i.e. is convex).
\subsection{Quadratic programming formulation}
We want some way to express the position of the hyperplanes, since there is one hyperplane orthogonal to each dimension, we can think of them as split of the values in dimension $d$. This can be represented by having a binary vector for each dimension, where a $0$ encodes a value belonging to the lower part of the dimension and $1$ encodes a value belonging to the upper part of the dimension. Formally we have a set of variables $u_{i,d} \in \{0,1\}$ that is $0$ if value $r_{i,d}$ is in the lower part and $1$ if it is in the upper part. The constraints are then that as soon as one of the variables are $1$ all variables above that must also be $1$:
\begin{itemize}
\item For each $d \in [D]$ the following must hold $u_{\pi^d(1),d} \leq u_{\pi^d(2),d} \leq \dots \leq u_{\pi^d(N),d}$.
\end{itemize}
We will describe the target function as a sum over all $2^D$ cells. We let the variable $q_i^c$ determine if an individual $i$ is in cell $c \in \{0,1\}^D$ by the constraints $q_i^c \leq s(u_{i,d}, c_d)$ for $d \in [D]$ where
$$
s(a,b) = \left\{
\begin {array}{cr}
1 - a& \text{if } b = 0\\
a     & \text{if } b = 1
\end {array}
\right.
$$

This says that $q_i^c$ are only allowed to be $1$ if $u_{i,d} = c_d$ for all $d \in [D]$. Note that it is no problem to use the $s$-function since we are summing over cells and we therefore now what the value of $b$ is.
For convinience let us define the number of cases and controls in a cell $c$ by
$$q_p^c = \sum_{i \in P} q_i^c,\quad q_n^c = \sum_{i \in C} q_i^c$$
We may then write our target function as:
$$f(\cdot) = -2^{2D}N^3\sum_{i \in [N],d \in [D]} (u_{i,d} - \frac{1}{2})^2 - 2^DN^2\sum_{i \in [N],c \in \{0,1\}^D} q_i^c + \sum_{c \in \{0,1\}^D}q_p^cq_n^c $$
The first term says that we want the $u_{i,d}$ to be either $0$ or $1$. The second term says that we each $q_i^c$ must be $1$ if it can. Finally, the last term says that we want each cell to discriminate between cases and controls as well as possible.

An intuitive argument for why this works goes as follows. Imagine that we have any function $h(x)$ that is bounded $0 \leq h(x) \leq L$. The idea behind the first two terms in the sum above is that they create peaks at certain points in the function $h(x)$, since the peaks are higher than $L$ any optimal solution must lie at these peaks. The peaks are equally high thus the optimal solution must be at the peak where $h(x)$ is greatest. There are some more conditions and practical problems, like when two solutions are infinitely close or when $h(x)$ has a large derivative but for sake of clarity I'll leave them out.
\subsection{Quadratic programming vectors and matrices}
This section is quite messy, I just want to show the principle. Let us now try to formulate the $Q$ matrix and $\mathbf{c}$ vector of the quadratic programming problem. The matrix $Q$ helps us to define all degree $2$ terms (e.g. $x^2$, $xy$) and $c$ all degree $1$ terms (e.g $x$). Let us take the first term of the target function
$$ -2^{2D}N^3\sum_{i \in [N],d \in [D]} (u_{i,d} - \frac{1}{2})^2 = -2^{2D}N^3\sum_{i \in [N],d \in [D]} u_{i,d}^2 - \frac{1}{2} u_{i,d} + \frac{1}{4}$$
If we stack the $u_{i,d}$ variables in a vector $\mathbf{u}$ of length $ND$ we can write the first term as
$$-2^{2D}N^3\sum_{i \in [N],d \in [D]} u_{i,d}^2 - \frac{1}{2} u_{i,d} + \frac{1}{4} = \mathbf{u}^T(-2^{2D}N^3I)\mathbf{u} - \frac{1}{2}\mathbf{e}_{u}^T\mathbf{u} + ND\frac{1}{4} $$ where $I$ is the identity matrix.
We do a similar process for the $q_i^c$ variables, let $\mathbf{q}$ be a vector of length $2^DN$ where the $Q_i^c$ variables are stacked. Let $B$ be a $2^DN \times 2^DN$ matrix so that in a row corresponding to $q_i^c$ where $i \in P$ it should have ones in columns $c \dot N + j$ for $j \in C$, and zeros in the rest of the columns. If $i \in C$ it should only contain zeros. We arrive at
$$- N^2\sum_{i \in [N],c \in \{0,1\}^D} q_i^c + \sum_{c \in \{0,1\}^D}q_p^cq_n^c = \mathbf{q}^TB\mathbf{q} -N^2\mathbf{e}_q^T\mathbf{q} $$
In block form we can write the whole quadratic programming problem as
\begin{equation*}
\mathbf{x} = 
 \begin{pmatrix}
  \mathbf{u} \\
  \mathbf{q} \\
 \end{pmatrix}
\end{equation*}

\begin{equation*}
 Q =
 \begin{pmatrix}
  -2^{2D}N^3I & \mathbf{0} \\
  \mathbf{0} & B \\
 \end{pmatrix}
\end{equation*}

\begin{equation*}
\mathbf{c} = 
 \begin{pmatrix}
  \frac{1}{2}\mathbf{e}_u \\
  -N^2\mathbf{e}_q \\
 \end{pmatrix}
\end{equation*}

Which results in
$$ f(\mathbf{x}) = \mathbf{x}^TQ\mathbf{x} + \mathbf{c}^T\mathbf{x} $$
The formulation of the constraints are done in a similar fashion.
%\section{Proof of maximization}
%\subsection{Theorem}
%Suppose we want to optimize a function $f(x)$ where $x \in [0,1]^n$, and $f(x)$ is bounded $0 \leq f(x) \leq L$. If 
%$$q(x) = \frac{4}{n}\sum_{i=1}^n (x_i - \frac{1}{2})^2$$
%then for any $\epsilon \in (0, \frac{1}{2})$
%$$ \operatorname*{arg\,max}_{x \in [0,1]^k} \frac{L}{\epsilon}q(x) + f(x) = \operatorname*{arg\,max}_{x \in [0, \epsilon]^k \cup [1-\epsilon, 1]^k} f(x)$$
%\subsection{Proof}
%Let $x \in [0, 1]$ and $x'$ be $x$ where each component in $(\epsilon, 1 - \epsilon)$ has been replaced by the closest of $1$ and $0$.

%Since q(x) is increasing and q(x) < 1, we can choose q(x) < \epsilon < 1and a x' such that q(x') - q(x) = \epsilon


%\section{Proof of equivalent maximization}
%\subsection{Theorem}
%Let $C$ represent a set of cells in $\mathbb{R}^k$ such that $\cup_{c \in C} c = \mathbb{R}^k$. Then for the random variables $X^+$ and $X^-$ the following is true
%$$ \operatorname*{arg\,max}_{C} Pr[c(X^+) \neq c(X^-)] = \operatorname*{arg\,max}_{C} Pr[h(X^+) > h(X^-)]$$
%where
%$$ h(X) =  \frac{Pr[c(X^+) = c(X)]}{Pr[c(X^-)=c(X)]}$$
%\subsection{Proof}
%The left side can be rewritten as follows
%$$Pr[c(X^+) \neq c(X^-)] = \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[c(X^+) = c_1, c(X^-) = c_2] $$
%since the left side is just a union of disjoint events.
%The right side can similarly be rewritten
%$$ Pr[h(X^+) > h(X^-)] = $$
%$$ = \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[h(X^+) > h(X^-) | c(X^+) = c_1, c(X^-) = c_2]Pr[c(X^+) = c_1, c(X^-) = c_2]$$
%$$ = \sum_{c_1,c_2 \in C, c_1 \neq c_2} I_{ \frac{Pr[c(X^+) = c_1]}{Pr[c(X^-) = c_1]} > \frac{Pr[c(X^+) = c_2]}{Pr[c(X^-) = c_2]} } Pr[c(X^+) = c_1, c(X^-) = c_2] =$$
%$$ = \frac{1}{2} \sum_{c_1,c_2 \in C, c_1 \neq c_2} Pr[c(X^+) = c_1, c(X^-) = c_2] $$
%The last step is  true since we are trying all pairs of cells and in one direction the indicator function must be true. Except on a factor $\frac{1}{2}$ the functions are the same, thus the theorem holds.
\end{document}
